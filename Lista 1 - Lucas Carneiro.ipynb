{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lista 1 - Lucas Carneiro"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Organizando os dados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "import nltk\n",
    "from nltk.corpus import machado, mac_morpho\n",
    "from nltk.tokenize import WordPunctTokenizer\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem.snowball import PortugueseStemmer\n",
    "\n",
    "import string\n",
    "\n",
    "#import enchant\n",
    "#PyEnchant não possui suporte para Windows x64\n",
    "\n",
    "from collections import defaultdict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\Casa\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package machado to\n",
      "[nltk_data]     C:\\Users\\Casa\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package machado is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('stopwords')\n",
    "nltk.download('machado')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "textos = []\n",
    "for i in machado.fileids():\n",
    "    textos.append(machado.raw(i))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Organizando o texto"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "swu = stopwords.words('portuguese') + list (string.punctuation)\n",
    "stemmer = PortugueseStemmer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "textos_limpos = []\n",
    "for texto in textos:\n",
    "    tlimpo = [token.lower() for token in WordPunctTokenizer().tokenize(texto) if token not in swu]\n",
    "    textos_limpos.append(tlimpo)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "textos_limpos2 = []\n",
    "for texto in textos_limpos:\n",
    "    tlimpo2 = [stemmer.stem(token) for token in texto]\n",
    "    textos_limpos2.append(tlimpo2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['conto', 'contos', 'fluminenses', '1870', 'contos', 'fluminenses', 'texto', 'fonte', 'obra', 'completa']\n",
      "['cont', 'cont', 'fluminens', '1870', 'cont', 'fluminens', 'text', 'font', 'obra', 'complet']\n"
     ]
    }
   ],
   "source": [
    "print(textos_limpos[0][0:10])\n",
    "print(textos_limpos2[0][0:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Gerando indices invertidos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "indice = defaultdict(lambda:set([]))\n",
    "for tid,t in enumerate(textos_limpos):\n",
    "    for term in t:\n",
    "        indice[term].add(tid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "indice2 = defaultdict(lambda:set([]))\n",
    "for tid,t in enumerate(textos_limpos2):\n",
    "    for term in t:\n",
    "        indice2[term].add(tid)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Função de busca"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def busca(search, indice):\n",
    "    \n",
    "    #operadores disponiveis:\n",
    "    ##'and'\n",
    "    ##'or'\n",
    "    ##'not'  \n",
    "    \n",
    "    #os operadores são executados em sequência\n",
    "    #o operador segue funcional até outro operador ser sugerido\n",
    "    #o operador padrão é o 'or'\n",
    "    \n",
    "    tokens = WordPunctTokenizer().tokenize(search)\n",
    "\n",
    "    resultado = set()\n",
    "    operador = 'or'\n",
    "    \n",
    "    for token in tokens:\n",
    "        if token == 'and':\n",
    "            operador = 'and'\n",
    "            \n",
    "        elif token == 'or':\n",
    "            operador = 'or'\n",
    "        \n",
    "        elif token == 'not':\n",
    "            operador = 'not'\n",
    "            \n",
    "        else:\n",
    "            resultado_temp = indice[token]\n",
    "            if operador == 'and':\n",
    "                resultado = resultado & resultado_temp\n",
    "            \n",
    "            elif operador == 'not':\n",
    "                resultado = resultado - resultado_temp\n",
    "                \n",
    "            else:\n",
    "                resultado = resultado | resultado_temp\n",
    "    \n",
    "    return resultado"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercício 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "O conjunto relevante considerado é a busca de termos stemizados no índice stemizado.\n",
    "A função a seguir retorna a Revocação e a Precisão de uma busca"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def RevPre(parcial,geral):\n",
    "    \n",
    "    #parcial = 'documentos encontrados'\n",
    "    #geral = 'documentos relevantes'\n",
    "    \n",
    "    rev = len(parcial & geral)/len(geral)\n",
    "    pre = len(parcial & geral)/len(parcial)\n",
    "    \n",
    "    print('Revocação = ' + str(rev))\n",
    "    print('Precisão = ' + str(pre))\n",
    "    \n",
    "    return rev,pre"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Revocação = 0.65625\n",
      "Precisão = 1.0\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(0.65625, 1.0)"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "palavra = 'ator'\n",
    "\n",
    "resultado = busca(stemmer.stem(palavra),indice2)\n",
    "resultado1 = busca(palavra,indice)\n",
    "\n",
    "RevPre(resultado1,resultado)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercício 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Revocação = 1.0\n",
      "Precisão = 0.9142857142857143\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(1.0, 0.9142857142857143)"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "equi = ['ator', 'atores','atriz','atrizes']\n",
    "\n",
    "resultado2 = set()\n",
    "for palavra in equi:\n",
    "    resultado2 = resultado2 | busca(palavra,indice)\n",
    "    \n",
    "RevPre(resultado2,resultado)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Houve uma grande melhora na revocação ao custo de uma pequena precisão"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercício 3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Redefinindo a função de busca"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def busca_corr_sup(termo,ind,dic):\n",
    "\n",
    "    #Função suporte de busca_corr\n",
    "    #faz a verificação ortográfica e a busca sobre os termos possíveis\n",
    "    \n",
    "    if not corretor.check(termo):\n",
    "        resultado = set()\n",
    "        for token in dic.suggest(termo):\n",
    "            resulado = resultado | ind[token]\n",
    "    else:\n",
    "        resultado = ind[termo]\n",
    "        \n",
    "    return resultado\n",
    "    \n",
    "def busca_corr(search, indice):\n",
    "    \n",
    "    #operadores disponiveis:\n",
    "    ##'and'\n",
    "    ##'or'\n",
    "    ##'not'  \n",
    "    \n",
    "    #os operadores são executados em sequência\n",
    "    #o operador segue funcional até outro operador ser sugerido\n",
    "    #o operador padrão é o 'or'\n",
    "    \n",
    "    corretor = enchant.Dict(\"pt_BR\")\n",
    "    tokens = WordPunctTokenizer().tokenize(search)\n",
    "\n",
    "    resultado = set()\n",
    "    operador = 'or'\n",
    "    \n",
    "    for token in tokens:\n",
    "        if token == 'and':\n",
    "            operador = 'and'\n",
    "            \n",
    "        elif token == 'or':\n",
    "            operador = 'or'\n",
    "        \n",
    "        elif token == 'not':\n",
    "            operador = 'not'\n",
    "            \n",
    "        else:\n",
    "            \n",
    "            #caso um termo esteja escrito errado, faz-se a busca sobre todas as correções sugeridas\n",
    "            \n",
    "            resultado_temp = busca_corr_sup(token,indice,corretor)\n",
    "               \n",
    "            if operador == 'and':\n",
    "                resultado = resultado & resultado_temp\n",
    "            \n",
    "            elif operador == 'not':\n",
    "                resultado = resultado - resultado_temp\n",
    "                \n",
    "            else:\n",
    "                resultado = resultado | resultado_temp\n",
    "    \n",
    "    return resultado"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercício 4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Criando novo indice para consulta por frases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "indice_f = defaultdict(lambda: defaultdict(list))\n",
    "\n",
    "for texto_id,texto in enumerate(textos_limpos2):\n",
    "    for termo_id,termo in enumerate(texto):\n",
    "        indice_f[termo][texto_id].append(termo_id)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Criando nova função de busca"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def prox_pos(lista1,lista2,raio):\n",
    "    \n",
    "    #Compara a proximidade entre termos de duas listas e devolve a média dos valores quando proximos\n",
    "    #Por que a média? Para a captar termos que estejam pertos, pelo menos, de boa parte do conjunto de tokens próximos\n",
    "    #Aumentando a revocação em sacrifício da precisão\n",
    "    \n",
    "    resultado = []\n",
    "    \n",
    "    for i in lista1:\n",
    "        for j in lista2:\n",
    "            if abs(i-j) <= raio:\n",
    "                resultado.append(round((i+j)/2))\n",
    "                \n",
    "    return resultado\n",
    "\n",
    "def busca_f(frase, indice, raio = 1):\n",
    "    \n",
    "    #frase = frase a ser buscada\n",
    "    #indice = indice invertido\n",
    "    #raio = distancia maxima entre os termos\n",
    "    \n",
    "    tokens = [stemmer.stem(token.lower())  for token in WordPunctTokenizer().tokenize(frase) if token not in swu]\n",
    "\n",
    "    #Verificar quais textos possuem todos os termos\n",
    "    textos_comuns = indice[tokens[0]].keys()\n",
    "    for token in tokens[1:]:\n",
    "        textos_comuns = textos_comuns & indice[token].keys()\n",
    "    \n",
    "    #Comparar as posições dentro dos textos em comum\n",
    "    resultado = defaultdict(list)\n",
    "    for texto in textos_comuns:\n",
    "        \n",
    "        resultado_temp = indice[tokens[0]][texto]\n",
    "        for token in tokens[1:]:\n",
    "            resultado_temp = prox_pos(resultado_temp,indice[token][texto],raio)\n",
    "            \n",
    "        if resultado_temp:\n",
    "            resultado[texto] = resultado_temp\n",
    "            \n",
    "    return resultado"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Testando a nova função busca"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "defaultdict(list, {230: [2, 4, 112, 136, 144, 164, 17460, 17462, 38852]})"
      ]
     },
     "execution_count": 106,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "busca_f('Dom Casmurro',indice_f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Obs: provavelmente o texto 230 é a obra \"Dom Casmurro\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercício 5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A ideia é criar a nova função de busca reduzindo os termos procurados, dando prioridade aos termos que estiverem na frente da frase procurada"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def busca_f_adv(frase, indice, raio = 1):\n",
    "    \n",
    "    #frase = frase a ser buscada\n",
    "    #indice = indice invertido\n",
    "    #raio = distancia maxima entre os termos\n",
    "    \n",
    "    tokens = [stemmer.stem(token.lower())  for token in WordPunctTokenizer().tokenize(frase) if token not in swu]\n",
    "\n",
    "    #Verificar quais textos possuem todos os termos\n",
    "    textos_comuns = indice[tokens[0]].keys()\n",
    "    for token in tokens[1:]:\n",
    "        textos_comuns = textos_comuns & indice[token].keys()\n",
    "\n",
    "    #Ainda que a busca seja reduzida os termos são procurados nos textos que possuem todas os itens buscados\n",
    "    #Por que?  para que uma busca específica não seja transformada em outra distante do objetivo original\n",
    "    while tokens:\n",
    "        \n",
    "        #Comparar as posições dentro dos textos em comum\n",
    "        resultado = defaultdict(list)\n",
    "        for texto in textos_comuns:\n",
    "\n",
    "            resultado_temp = indice[tokens[0]][texto]\n",
    "            for token in tokens[1:]:\n",
    "                resultado_temp = prox_pos(resultado_temp,indice[token][texto],raio)\n",
    "\n",
    "            if resultado_temp:\n",
    "                resultado[texto] = resultado_temp\n",
    "\n",
    "        if not resultado:\n",
    "            tokens = tokens[:-1]\n",
    "        else:  \n",
    "            return resultado"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "defaultdict(list,\n",
       "            {0: [21159, 28210],\n",
       "             1: [1973],\n",
       "             2: [467, 5288, 12274, 23175],\n",
       "             3: [10668, 21399],\n",
       "             5: [11155],\n",
       "             12: [9069],\n",
       "             24: [499, 520, 1971],\n",
       "             33: [1031, 1082, 1150],\n",
       "             34: [242],\n",
       "             39: [9774],\n",
       "             41: [2344],\n",
       "             43: [1500],\n",
       "             49: [1255],\n",
       "             51: [2024],\n",
       "             52: [491],\n",
       "             62: [4043],\n",
       "             71: [4522],\n",
       "             94: [519],\n",
       "             140: [8166],\n",
       "             175: [178, 1022],\n",
       "             182: [2452],\n",
       "             185: [21064, 26008, 30803],\n",
       "             186: [667, 1634],\n",
       "             188: [3468, 6056, 6178],\n",
       "             191: [1397,\n",
       "              14432,\n",
       "              14462,\n",
       "              14483,\n",
       "              14503,\n",
       "              14531,\n",
       "              14548,\n",
       "              14580,\n",
       "              14597,\n",
       "              14646,\n",
       "              14668,\n",
       "              14722,\n",
       "              14838,\n",
       "              14857,\n",
       "              14897,\n",
       "              14920,\n",
       "              14938,\n",
       "              14995,\n",
       "              15012,\n",
       "              17294],\n",
       "             192: [1465, 11748],\n",
       "             193: [75062, 104344],\n",
       "             217: [5037],\n",
       "             219: [13063, 13099, 13105],\n",
       "             221: [155, 8271, 8467, 9816],\n",
       "             224: [5208],\n",
       "             225: [3839, 4456, 4667],\n",
       "             226: [17361],\n",
       "             227: [31950],\n",
       "             229: [1791, 32496, 39627],\n",
       "             230: [1, 4, 111, 135, 143, 164, 196, 1457, 17461, 38852],\n",
       "             232: [64, 1956, 11980, 13741, 16493],\n",
       "             233: [2972],\n",
       "             236: [3744],\n",
       "             237: [110],\n",
       "             238: [3359],\n",
       "             240: [912, 3841],\n",
       "             244: [42122],\n",
       "             245: [20334]})"
      ]
     },
     "execution_count": 107,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "busca_f_adv('Dom conto rua',indice_f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
